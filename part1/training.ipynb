{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.load('data1.npy')\n",
    "lab1 = np.load('lab1.npy')\n",
    "data0 = np.load('data0.npy')\n",
    "lab0 = np.load('lab0.npy')\n",
    "data2 = np.load('data2.npy')\n",
    "lab2 = np.load('lab2.npy')\n",
    "total_data=np.concatenate((data0,data1,data2),axis=0)\n",
    "total_lab=np.concatenate((lab0,lab1,lab2),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = len(total_data)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = int(0.1 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "train_data = total_data[:train_size]\n",
    "train_lab=total_lab[:train_size]\n",
    "val_data = total_data[train_size:train_size + val_size]\n",
    "val_lab=total_lab[train_size:train_size + val_size]\n",
    "test_data = total_data[train_size + val_size:]\n",
    "test_lab=total_lab[train_size + val_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch data shape: torch.Size([2048, 1, 40, 168])\n",
      "Batch labels shape: torch.Size([2048])\n"
     ]
    }
   ],
   "source": [
    "train_data = torch.tensor(train_data, dtype=torch.float32)\n",
    "train_lab = torch.tensor(train_lab, dtype=torch.long)\n",
    "\n",
    "val_data = torch.tensor(val_data, dtype=torch.float32)\n",
    "val_lab = torch.tensor(val_lab, dtype=torch.long)\n",
    "\n",
    "test_data = torch.tensor(test_data, dtype=torch.float32)\n",
    "test_lab = torch.tensor(test_lab, dtype=torch.long)\n",
    "\n",
    "# Add channel dimension (1 for grayscale images)\n",
    "train_data = train_data.unsqueeze(1)  # Shape becomes [num_samples, 1, height, width]\n",
    "val_data = val_data.unsqueeze(1)\n",
    "test_data = test_data.unsqueeze(1)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(train_data, train_lab)\n",
    "val_dataset = TensorDataset(val_data, val_lab)\n",
    "test_dataset = TensorDataset(test_data, test_lab)\n",
    "\n",
    "# Define data loaders\n",
    "batch_size = 2048\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check a single batch\n",
    "for batch_data, batch_labels in train_loader:\n",
    "    print(\"Batch data shape:\", batch_data.shape)  # Should be [batch_size, 1, height, width]\n",
    "    print(\"Batch labels shape:\", batch_labels.shape)  # Should be [batch_size]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return F.relu(x)\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.block1 = ConvBlock(1, 32)\n",
    "        self.block2 = ConvBlock(32, 64)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Calculate the flattened size dynamically based on the input shape\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *input_shape)  # Batch size of 1 for testing\n",
    "            flattened_size = self._get_flattened_size(dummy_input)\n",
    "\n",
    "        self.fc1 = nn.Linear(flattened_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)  # Output a single value for regression\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def _get_flattened_size(self, x):\n",
    "        x = self.pool(self.block1(x))\n",
    "        x = self.pool(self.block2(x))\n",
    "        return x.view(-1).size(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.block1(x))\n",
    "        x = self.pool(self.block2(x))\n",
    "\n",
    "        # Flatten for fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)  # Output shape: [batch_size, 1]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(set(total_lab))  # Replace with the number of unique labels\n",
    "input_shape = (1, 40, 168) \n",
    "model = CNNModel(input_shape)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss: 138.6424\n",
      "Validation Loss: 47.4990\n",
      "----------------------------------------\n",
      "Epoch 2\n",
      "Training Loss: 53.6347\n",
      "Validation Loss: 36.3398\n",
      "----------------------------------------\n",
      "Epoch 3\n",
      "Training Loss: 44.8591\n",
      "Validation Loss: 33.1168\n",
      "----------------------------------------\n",
      "Epoch 4\n",
      "Training Loss: 41.3626\n",
      "Validation Loss: 32.4657\n",
      "----------------------------------------\n",
      "Epoch 5\n",
      "Training Loss: 39.4821\n",
      "Validation Loss: 30.9911\n",
      "----------------------------------------\n",
      "Epoch 6\n",
      "Training Loss: 38.5358\n",
      "Validation Loss: 29.8522\n",
      "----------------------------------------\n",
      "Epoch 7\n",
      "Training Loss: 37.4814\n",
      "Validation Loss: 29.9637\n",
      "----------------------------------------\n",
      "Epoch 8\n",
      "Training Loss: 36.4803\n",
      "Validation Loss: 28.6589\n",
      "----------------------------------------\n",
      "Epoch 9\n",
      "Training Loss: 35.5161\n",
      "Validation Loss: 28.4227\n",
      "----------------------------------------\n",
      "Epoch 10\n",
      "Training Loss: 34.9504\n",
      "Validation Loss: 27.4909\n",
      "----------------------------------------\n",
      "Epoch 11\n",
      "Training Loss: 33.8780\n",
      "Validation Loss: 28.0225\n",
      "----------------------------------------\n",
      "Epoch 12\n",
      "Training Loss: 32.9238\n",
      "Validation Loss: 25.6974\n",
      "----------------------------------------\n",
      "Epoch 13\n",
      "Training Loss: 32.2477\n",
      "Validation Loss: 27.6305\n",
      "----------------------------------------\n",
      "Epoch 14\n",
      "Training Loss: 31.4401\n",
      "Validation Loss: 25.2235\n",
      "----------------------------------------\n",
      "Epoch 15\n",
      "Training Loss: 31.0203\n",
      "Validation Loss: 26.4553\n",
      "----------------------------------------\n",
      "Epoch 16\n",
      "Training Loss: 30.7797\n",
      "Validation Loss: 24.0256\n",
      "----------------------------------------\n",
      "Epoch 17\n",
      "Training Loss: 29.6738\n",
      "Validation Loss: 23.7208\n",
      "----------------------------------------\n",
      "Epoch 18\n",
      "Training Loss: 28.9791\n",
      "Validation Loss: 22.7865\n",
      "----------------------------------------\n",
      "Epoch 19\n",
      "Training Loss: 29.1474\n",
      "Validation Loss: 22.1118\n",
      "----------------------------------------\n",
      "Epoch 20\n",
      "Training Loss: 28.1116\n",
      "Validation Loss: 21.5326\n",
      "----------------------------------------\n",
      "Epoch 21\n",
      "Training Loss: 27.3925\n",
      "Validation Loss: 26.2344\n",
      "----------------------------------------\n",
      "Epoch 22\n",
      "Training Loss: 26.9592\n",
      "Validation Loss: 22.1035\n",
      "----------------------------------------\n",
      "Epoch 23\n",
      "Training Loss: 26.4533\n",
      "Validation Loss: 24.9445\n",
      "----------------------------------------\n",
      "Epoch 24\n",
      "Training Loss: 25.7025\n",
      "Validation Loss: 19.3427\n",
      "----------------------------------------\n",
      "Epoch 25\n",
      "Training Loss: 25.7774\n",
      "Validation Loss: 24.2614\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate model function for regression\n",
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    running_loss = 0.0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device).float()\n",
    "            outputs = model(images).squeeze(1)  # Flatten outputs to [batch_size]\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = running_loss / len(val_loader)\n",
    "    return avg_val_loss, predictions, targets\n",
    "\n",
    "# Train model\n",
    "def train_model(model, train_loader, val_loader, optimizer, epochs=50):\n",
    "    # wandb.init(\n",
    "    #     project=\"MLNS-pre\",\n",
    "    #     name=f\"cnn-training-regression-{wandb.util.generate_id()}\",\n",
    "    #     tags=[\"cnn\", \"pytorch\", \"regression\"]\n",
    "    # )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images).squeeze(1)  # Flatten outputs to [batch_size]\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Average training loss\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss, _, _ = evaluate_model(model, val_loader)\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        # wandb.log({\n",
    "        #     \"epoch\": epoch + 1,\n",
    "        #     \"train_loss\": train_loss,\n",
    "        #     \"val_loss\": val_loss\n",
    "        # })\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "        print(f\"Training Loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        print(\"----------------------------------------\")\n",
    "\n",
    "    # wandb.finish()\n",
    "\n",
    "# Test model\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    test_loss = 0.0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device).float()\n",
    "            outputs = model(images).squeeze(1)  # Flatten outputs to [batch_size]\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    rounded_predictions = np.round(predictions)  # Round predictions to nearest integer\n",
    "\n",
    "    # Calculate accuracy\n",
    "    correct_predictions = np.sum(rounded_predictions == targets)\n",
    "    accuracy = correct_predictions / len(targets) * 100\n",
    "\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "train_model(model, train_loader, val_loader, optimizer, epochs=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 23.9763\n",
      "Accuracy: 7.97%\n"
     ]
    }
   ],
   "source": [
    "test_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to regression_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model after training\n",
    "torch.save(model.state_dict(), \"regression_model.pth\")\n",
    "print(\"Model saved to regression_model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sanchit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
